========================================================
GUARANTEED FINAL RUN & AGGREGATION
Job ID: 118623
Node: gpu02
Start Time: Fri Jun  6 06:48:42 PDT 2025
This script runs the final models with definitive fixes.
========================================================
Loading TensorFlow/20231212 module...
Setting environment to prioritize local packages...
Ensuring latest compatible packages are installed...
GPU Information:
Fri Jun  6 06:48:51 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:D8:00.0 Off |                    0 |
| N/A   28C    P0             25W /  250W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Starting Final Aggregation script with guaranteed fixes...
Command: python run_and_aggregate.py
üéØ Starting Final Run & Aggregation Process
üöÄ Models to run: deberta, electra
=== RUNNING TRANSFORMER EXPERIMENTS ===

Loading data...
Train data shape: (78189, 8)
Test data shape: (16755, 8)
Test labels shape: (153164, 7)
Preprocessing data...
Test data already contains labels, using them directly.
After preprocessing - Train: (78189, 9), Test: (16755, 9)
Creating data splits...
Train split: 62,551 samples
Validation split: 15,638 samples
Test split: 16,755 samples
Training 2 models: deberta, electra

============================================================
Training Model 1/2: microsoft/deberta-base
Key: deberta
Batch Size: 32, Max Length: 512, Epochs: 8
============================================================
Creating DataLoaders...
Created DataLoaders - Train: 1955 batches, Val: 489 batches, Test: 524 batches

=== Training microsoft/deberta-base ===
Attempting to load microsoft/deberta-base with safetensors...
‚úÖ Successfully loaded microsoft/deberta-base with safetensors
Training on device: cuda
Mixed precision training: DISABLED for this model
Number of parameters: 138,606,342
Training for 8 epochs...
Early stopping patience: 3 epochs

=== Epoch 1/8 ===
‚ùå Error training deberta: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 62.19 MiB is free. Including non-PyTorch memory, this process has 31.67 GiB memory in use. Of the allocated memory 31.21 GiB is allocated by PyTorch, and 98.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Continuing with next model...

============================================================
Training Model 2/2: google/electra-base-discriminator
Key: electra
Batch Size: 32, Max Length: 512, Epochs: 8
============================================================
Creating DataLoaders...
Created DataLoaders - Train: 1955 batches, Val: 489 batches, Test: 524 batches

=== Training google/electra-base-discriminator ===
Attempting to load google/electra-base-discriminator with safetensors...
‚úÖ Successfully loaded google/electra-base-discriminator with safetensors
Training on device: cuda
Mixed precision training: ENABLED
Number of parameters: 108,896,262
Training for 8 epochs...
Early stopping patience: 3 epochs

=== Epoch 1/8 ===
Train Loss: 0.6937
Val Loss: 0.6950
Val F1 (Micro): 0.0589
Val F1 (Macro): 0.0434
Val ROC-AUC (Micro): 0.4218
New best model saved! F1: 0.0589

=== Epoch 2/8 ===
Train Loss: 0.5570
Val Loss: 0.3905
Val F1 (Micro): 0.0117
Val F1 (Macro): 0.0131
Val ROC-AUC (Micro): 0.5267
No improvement. Patience: 1/3

=== Epoch 3/8 ===
Train Loss: 0.2435
Val Loss: 0.1664
Val F1 (Micro): 0.0000
Val F1 (Macro): 0.0000
Val ROC-AUC (Micro): 0.8179
No improvement. Patience: 2/3

=== Epoch 4/8 ===
Train Loss: 0.1540
Val Loss: 0.1161
Val F1 (Micro): 0.0000
Val F1 (Macro): 0.0000
Val ROC-AUC (Micro): 0.9565
No improvement. Patience: 3/3
Early stopping triggered! Best F1: 0.0589

Training completed. Best F1: 0.0589 at epoch 1
Training history plot saved to /WAVE/users2/unix/ysuryavanshi/NLP Project/plots/google_electra-base-discriminator_training_history.png

=== Evaluating google/electra-base-discriminator on Test Set ===
Loaded best model checkpoint
Test Loss: 0.6947
Test F1 (Micro): 0.0576
Test F1 (Macro): 0.0404
Test ROC-AUC (Micro): 0.4185

=== Per-Class Test F1 Scores ===
toxic: 0.0074
severe_toxic: 0.0217
obscene: 0.0863
threat: 0.0067
insult: 0.0930
identity_hate: 0.0271
‚úÖ ELECTRA completed successfully!
   Best F1: 0.0589
   Test F1-Micro: 0.0576

=== TRANSFORMER EXPERIMENTS COMPLETED ===
Successfully trained 1 out of 2 models

‚úÖ Original model configuration restored.

üîç Loading all existing and new results for final report...
  - Found existing result for 'bert' at /WAVE/users2/unix/ysuryavanshi/NLP Project/results/bert-base-uncased_test_metrics.csv
  - Found existing result for 'roberta' at /WAVE/users2/unix/ysuryavanshi/NLP Project/results/roberta-base_test_metrics.csv
  - WARNING: No result file found for 'deberta'
  - Found existing result for 'hatebert' at /WAVE/users2/unix/ysuryavanshi/NLP Project/results/martin-ha_toxic-comment-model_test_metrics.csv
  - Found new result for 'electra'

üìä Generating final comprehensive report for all models...
Creating comprehensive model comparison...
Comparison table saved to /WAVE/users2/unix/ysuryavanshi/NLP Project/results/all_models_comparison.csv

üìä MODEL PERFORMANCE RANKING:
----------------------------------------------------------------------
 3. ROBERTA    | F1-Micro: 0.7235 | F1-Macro: 0.3798 | ROC-AUC: 0.9796
 4. HATEBERT   | F1-Micro: 0.1473 | F1-Macro: 0.0958 | ROC-AUC: 0.9353
 2. BERT       | F1-Micro: 0.0745 | F1-Macro: 0.0319 | ROC-AUC: 0.5012
 1. ELECTRA    | F1-Micro: 0.0576 | F1-Macro: 0.0404 | ROC-AUC: 0.4185
Comprehensive comparison plot saved to /WAVE/users2/unix/ysuryavanshi/NLP Project/plots/comprehensive_transformer_comparison.png

üéâ Process Complete!
Exit status: 0
========================================================
GUARANTEED FINAL RUN FINISHED at Fri Jun  6 07:51:23 PDT 2025
========================================================
