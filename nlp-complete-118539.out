Job ID: 118539
Job Name: nlp-complete-30h
Node: gpu01
Start Time: Thu Jun  5 01:06:48 PDT 2025
Working Directory: /WAVE/users2/unix/ysuryavanshi/NLP Project
Configuration: COMPLETE COMPREHENSIVE ANALYSIS
Resource allocation: 2 CPUs, 8GB memory total
Training: ALL 5 MODELS (BERT, RoBERTa, DeBERTa, HateBERT, ELECTRA)
Settings: 10 epochs each, batch size 128, full 512 token length
Time allocation: 30 hours (estimated completion: ~23 hours)
Loading TensorFlow module...
GPU Information:
Thu Jun  5 01:06:48 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:D8:00.0 Off |                    0 |
| N/A   31C    P0             26W /  250W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Starting COMPREHENSIVE NLP Toxic Comment Classification...
This will train all baseline and advanced models with full evaluation
Models: Logistic Regression, Random Forest, BERT, RoBERTa, DeBERTa, HateBERT, ELECTRA
Command: python main.py --all
==========================================
STARTING COMPLETE PIPELINE at Thu Jun  5 01:06:48 PDT 2025
==========================================
=== SETTING UP ENVIRONMENT ===
Using device: cuda
GPU: Tesla V100-PCIE-32GB
GPU Memory: 34.1 GB
Environment setup complete!

=== RUNNING DATA ANALYSIS ===
Loading data...
Train data shape: (78189, 8)
Test data shape: (16755, 8)
Test labels shape: (153164, 7)
Preprocessing data...
Test data already contains labels, using them directly.
After preprocessing - Train: (78189, 9), Test: (16755, 9)
Analyzing dataset...
Dataset analysis plot saved to /WAVE/users2/unix/ysuryavanshi/NLP Project/plots/dataset_analysis.png

=== Dataset Summary ===
Total training samples: 78,189
Average comment length: 387.7 characters
Median comment length: 201.0 characters
Percentage of toxic comments: 9.58%
Percentage of clean comments: 89.79%

=== Label Statistics ===
toxic: 7,494 (9.58%)
severe_toxic: 780 (1.00%)
obscene: 4,149 (5.31%)
threat: 247 (0.32%)
insult: 3,861 (4.94%)
identity_hate: 686 (0.88%)
Data analysis complete!

Running baseline experiments...
=== RUNNING BASELINE EXPERIMENTS ===

Loading data...
Train data shape: (78189, 8)
Test data shape: (16755, 8)
Test labels shape: (153164, 7)
Preprocessing data...
Test data already contains labels, using them directly.
After preprocessing - Train: (78189, 9), Test: (16755, 9)
Creating data splits...
Train split: 62,551 samples
Validation split: 15,638 samples
Test split: 16,755 samples
Creating TF-IDF features for baseline...
TF-IDF feature shape: (62551, 10000)
Training Logistic Regression...
Training Random Forest...
Evaluating logistic_regression...

=== LOGISTIC_REGRESSION RESULTS ===
Accuracy: 0.9152
F1-Score (Micro): 0.6383
F1-Score (Macro): 0.4565
ROC-AUC (Micro): 0.9727
ROC-AUC (Macro): 0.9699

=== Per-Class F1-Scores ===
toxic: 0.6924
severe_toxic: 0.2795
obscene: 0.7080
threat: 0.2353
insult: 0.6076
identity_hate: 0.2162
Evaluating random_forest...

=== RANDOM_FOREST RESULTS ===
Accuracy: 0.9110
F1-Score (Micro): 0.6874
F1-Score (Macro): 0.4217
ROC-AUC (Micro): 0.9645
ROC-AUC (Macro): 0.9392

=== Per-Class F1-Scores ===
toxic: 0.7310
severe_toxic: 0.1383
obscene: 0.7911
threat: 0.0984
insult: 0.6501
identity_hate: 0.1212
Baseline comparison plot saved to /WAVE/users2/unix/ysuryavanshi/NLP Project/plots/baseline_comparison.png
Making predictions on test set...
Predicting with logistic_regression...
logistic_regression Test F1 (Micro): 0.6477
logistic_regression Test F1 (Macro): 0.4552
Predicting with random_forest...
random_forest Test F1 (Micro): 0.6993
random_forest Test F1 (Macro): 0.4183

=== BASELINE EXPERIMENTS COMPLETED ===
Running transformer experiments...
=== RUNNING TRANSFORMER EXPERIMENTS ===

Loading data...
Train data shape: (78189, 8)
Test data shape: (16755, 8)
Test labels shape: (153164, 7)
Preprocessing data...
Test data already contains labels, using them directly.
After preprocessing - Train: (78189, 9), Test: (16755, 9)
Creating data splits...
Train split: 62,551 samples
Validation split: 15,638 samples
Test split: 16,755 samples
Training 5 models: bert, roberta, deberta, hatebert, electra

============================================================
Training Model 1/5: bert-base-uncased
Key: bert
Batch Size: 128, Max Length: 512, Epochs: 8
============================================================
Creating DataLoaders...
Created DataLoaders - Train: 489 batches, Val: 123 batches, Test: 131 batches

=== Training bert-base-uncased ===
Training on device: cuda
Number of parameters: 109,486,854
Training for 8 epochs...
Early stopping patience: 3 epochs

=== Epoch 1/8 ===
Train Loss: 0.6585
Val Loss: 0.6567
Val F1 (Micro): 0.0568
Val F1 (Macro): 0.0420
Val ROC-AUC (Micro): 0.4617
New best model saved! F1: 0.0568

=== Epoch 2/8 ===
Train Loss: 0.6263
Val Loss: 0.5910
Val F1 (Micro): 0.0746
Val F1 (Macro): 0.0287
Val ROC-AUC (Micro): 0.5010
New best model saved! F1: 0.0746

=== Epoch 3/8 ===
Train Loss: 0.5251
Val Loss: 0.4276
Val F1 (Micro): 0.0235
Val F1 (Macro): 0.0133
Val ROC-AUC (Micro): 0.5748
No improvement. Patience: 1/3

=== Epoch 4/8 ===
Train Loss: 0.3244
Val Loss: 0.2216
Val F1 (Micro): 0.0000
Val F1 (Macro): 0.0000
Val ROC-AUC (Micro): 0.7536
No improvement. Patience: 2/3

=== Epoch 5/8 ===
Train Loss: 0.1912
Val Loss: 0.1570
Val F1 (Micro): 0.0000
Val F1 (Macro): 0.0000
Val ROC-AUC (Micro): 0.8376
No improvement. Patience: 3/3
Early stopping triggered! Best F1: 0.0746

Training completed. Best F1: 0.0746 at epoch 2
Training history plot saved to /WAVE/users2/unix/ysuryavanshi/NLP Project/plots/bert-base-uncased_training_history.png

=== Evaluating bert-base-uncased on Test Set ===
Loaded best model checkpoint
Test Loss: 0.5908
Test F1 (Micro): 0.0745
Test F1 (Macro): 0.0319
Test ROC-AUC (Micro): 0.5012

=== Per-Class Test F1 Scores ===
toxic: 0.0272
severe_toxic: 0.0238
obscene: 0.0022
threat: 0.0101
insult: 0.1088
identity_hate: 0.0190
✅ BERT completed successfully!
   Best F1: 0.0746
   Test F1-Micro: 0.0745

============================================================
Training Model 2/5: roberta-base
Key: roberta
Batch Size: 128, Max Length: 512, Epochs: 8
============================================================
Creating DataLoaders...
Created DataLoaders - Train: 489 batches, Val: 123 batches, Test: 131 batches

=== Training roberta-base ===
Training on device: cuda
Number of parameters: 124,650,246
Training for 8 epochs...
Early stopping patience: 3 epochs

=== Epoch 1/8 ===
Train Loss: 0.7070
Val Loss: 0.7062
Val F1 (Micro): 0.1288
Val F1 (Macro): 0.0600
Val ROC-AUC (Micro): 0.7360
New best model saved! F1: 0.1288

=== Epoch 2/8 ===
Train Loss: 0.6962
Val Loss: 0.6849
Val F1 (Micro): 0.1358
Val F1 (Macro): 0.0466
Val ROC-AUC (Micro): 0.7411
New best model saved! F1: 0.1358

=== Epoch 3/8 ===
Train Loss: 0.6417
Val Loss: 0.5987
Val F1 (Micro): 0.1468
Val F1 (Macro): 0.0488
Val ROC-AUC (Micro): 0.7483
New best model saved! F1: 0.1468

=== Epoch 4/8 ===
Train Loss: 0.3046
Val Loss: 0.1510
Val F1 (Micro): 0.0000
Val F1 (Macro): 0.0000
Val ROC-AUC (Micro): 0.8235
No improvement. Patience: 1/3

=== Epoch 5/8 ===
Train Loss: 0.1398
Val Loss: 0.1079
Val F1 (Micro): 0.0831
Val F1 (Macro): 0.0308
Val ROC-AUC (Micro): 0.9543
No improvement. Patience: 2/3

=== Epoch 6/8 ===
Train Loss: 0.0988
Val Loss: 0.0806
Val F1 (Micro): 0.5042
Val F1 (Macro): 0.2140
Val ROC-AUC (Micro): 0.9724
New best model saved! F1: 0.5042

=== Epoch 7/8 ===
Train Loss: 0.0785
Val Loss: 0.0666
Val F1 (Micro): 0.6948
Val F1 (Macro): 0.3647
Val ROC-AUC (Micro): 0.9777
New best model saved! F1: 0.6948

=== Epoch 8/8 ===
Train Loss: 0.0672
Val Loss: 0.0610
Val F1 (Micro): 0.7230
Val F1 (Macro): 0.3784
Val ROC-AUC (Micro): 0.9799
New best model saved! F1: 0.7230

Training completed. Best F1: 0.7230 at epoch 8
Training history plot saved to /WAVE/users2/unix/ysuryavanshi/NLP Project/plots/roberta-base_training_history.png

=== Evaluating roberta-base on Test Set ===
Loaded best model checkpoint
Test Loss: 0.0600
Test F1 (Micro): 0.7235
Test F1 (Macro): 0.3798
Test ROC-AUC (Micro): 0.9796

=== Per-Class Test F1 Scores ===
toxic: 0.7778
severe_toxic: 0.0000
obscene: 0.7846
threat: 0.0000
insult: 0.7166
identity_hate: 0.0000
✅ ROBERTA completed successfully!
   Best F1: 0.7230
   Test F1-Micro: 0.7235

============================================================
Training Model 3/5: microsoft/deberta-base
Key: deberta
Batch Size: 96, Max Length: 512, Epochs: 8
============================================================
Creating DataLoaders...
Created DataLoaders - Train: 652 batches, Val: 163 batches, Test: 175 batches

=== Training microsoft/deberta-base ===
❌ Error training deberta: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.
See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434
Continuing with next model...

============================================================
Training Model 4/5: martin-ha/toxic-comment-model
Key: hatebert
Batch Size: 128, Max Length: 512, Epochs: 8
============================================================
Creating DataLoaders...
Created DataLoaders - Train: 489 batches, Val: 123 batches, Test: 131 batches

=== Training martin-ha/toxic-comment-model ===
❌ Error training hatebert: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.
See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434
Continuing with next model...

============================================================
Training Model 5/5: google/electra-base-discriminator
Key: electra
Batch Size: 128, Max Length: 512, Epochs: 8
============================================================
Creating DataLoaders...
Created DataLoaders - Train: 489 batches, Val: 123 batches, Test: 131 batches

=== Training google/electra-base-discriminator ===
❌ Error training electra: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.
See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434
Continuing with next model...

=== GENERATING COMPREHENSIVE COMPARISON ===
Creating comprehensive model comparison...
Comparison table saved to /WAVE/users2/unix/ysuryavanshi/NLP Project/results/all_models_comparison.csv

📊 MODEL PERFORMANCE RANKING:
----------------------------------------------------------------------
 2. ROBERTA    | F1-Micro: 0.7235 | F1-Macro: 0.3798 | ROC-AUC: 0.9796
 1. BERT       | F1-Micro: 0.0745 | F1-Macro: 0.0319 | ROC-AUC: 0.5012
Comprehensive comparison plot saved to /WAVE/users2/unix/ysuryavanshi/NLP Project/plots/comprehensive_transformer_comparison.png

=== TRANSFORMER EXPERIMENTS COMPLETED ===
Successfully trained 2 out of 5 models

Experiments completed in 8941.42 seconds
Generating comprehensive report...
=== GENERATING COMPREHENSIVE REPORT ===
Creating performance visualizations...
Comprehensive comparison plot saved to /WAVE/users2/unix/ysuryavanshi/NLP Project/plots/comprehensive_comparison.png
Per-class analysis plot saved to /WAVE/users2/unix/ysuryavanshi/NLP Project/plots/per_class_analysis.png
Generating detailed report...
Performance summary saved to /WAVE/users2/unix/ysuryavanshi/NLP Project/results/performance_summary.csv
Class difficulty analysis saved to /WAVE/users2/unix/ysuryavanshi/NLP Project/results/class_difficulty_analysis.csv

=== CLASS DIFFICULTY ANALYSIS ===
Classes ranked by difficulty (hardest first):
threat: Mean F1 = 0.0051 (±0.0051)
identity_hate: Mean F1 = 0.0095 (±0.0095)
severe_toxic: Mean F1 = 0.0119 (±0.0119)
obscene: Mean F1 = 0.3934 (±0.3912)
toxic: Mean F1 = 0.4025 (±0.3753)
insult: Mean F1 = 0.4127 (±0.3039)
Detailed report saved to /WAVE/users2/unix/ysuryavanshi/NLP Project/results/experiment_report.md
Comprehensive report generation completed!
Check the /WAVE/users2/unix/ysuryavanshi/NLP Project/results directory for all outputs.

=== TOTAL EXECUTION TIME: 8944.35 seconds ===
==========================================
COMPLETE PIPELINE FINISHED at Thu Jun  5 03:36:03 PDT 2025
==========================================
Exit status: 0

📁 RESULTS GENERATED:
   - Baseline models: Logistic Regression, Random Forest
   - Advanced models: BERT, RoBERTa, DeBERTa, HateBERT, ELECTRA
   - Comprehensive comparison and analysis
   - All plots and visualizations
   - Per-category performance breakdown

📊 Check results in:
   - results/ directory for all CSV files
   - plots/ directory for all visualizations
   - models/ directory for saved model checkpoints
