========================================================
GUARANTEED FINAL RUN & AGGREGATION
Job ID: 118622
Node: gpu02
Start Time: Fri Jun  6 06:43:40 PDT 2025
This script runs the final models with definitive fixes.
========================================================
Loading TensorFlow/20231212 module...
Setting environment to prioritize local packages...
Ensuring latest compatible packages are installed...
GPU Information:
Fri Jun  6 06:43:49 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:D8:00.0 Off |                    0 |
| N/A   28C    P0             25W /  250W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Starting Final Aggregation script with guaranteed fixes...
Command: python run_and_aggregate.py
üéØ Starting Final Run & Aggregation Process
üöÄ Models to run: deberta, electra
=== RUNNING TRANSFORMER EXPERIMENTS ===

Loading data...
Train data shape: (78189, 8)
Test data shape: (16755, 8)
Test labels shape: (153164, 7)
Preprocessing data...
Test data already contains labels, using them directly.
After preprocessing - Train: (78189, 9), Test: (16755, 9)
Creating data splits...
Train split: 62,551 samples
Validation split: 15,638 samples
Test split: 16,755 samples
Training 2 models: deberta, electra

============================================================
Training Model 1/2: microsoft/deberta-base
Key: deberta
Batch Size: 64, Max Length: 512, Epochs: 8
============================================================
Creating DataLoaders...
Created DataLoaders - Train: 978 batches, Val: 245 batches, Test: 262 batches

=== Training microsoft/deberta-base ===
Attempting to load microsoft/deberta-base with safetensors...
‚úÖ Successfully loaded microsoft/deberta-base with safetensors
Training on device: cuda
Mixed precision training: DISABLED for this model
Number of parameters: 138,606,342
Training for 8 epochs...
Early stopping patience: 3 epochs

=== Epoch 1/8 ===
‚ùå Error training deberta: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 674.19 MiB is free. Including non-PyTorch memory, this process has 31.07 GiB memory in use. Of the allocated memory 30.49 GiB is allocated by PyTorch, and 221.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Continuing with next model...

============================================================
Training Model 2/2: google/electra-base-discriminator
Key: electra
Batch Size: 64, Max Length: 512, Epochs: 8
============================================================
Creating DataLoaders...
Created DataLoaders - Train: 978 batches, Val: 245 batches, Test: 262 batches

=== Training google/electra-base-discriminator ===
Attempting to load google/electra-base-discriminator with safetensors...
‚úÖ Successfully loaded google/electra-base-discriminator with safetensors
Training on device: cuda
Mixed precision training: ENABLED
Number of parameters: 108,896,262
Training for 8 epochs...
Early stopping patience: 3 epochs

=== Epoch 1/8 ===
‚ùå Error training electra: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 660.19 MiB is free. Including non-PyTorch memory, this process has 31.08 GiB memory in use. Of the allocated memory 29.44 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Continuing with next model...

=== TRANSFORMER EXPERIMENTS COMPLETED ===
Successfully trained 0 out of 2 models

‚úÖ Original model configuration restored.

üîç Loading all existing and new results for final report...
  - Found existing result for 'bert' at /WAVE/users2/unix/ysuryavanshi/NLP Project/results/bert-base-uncased_test_metrics.csv
  - Found existing result for 'roberta' at /WAVE/users2/unix/ysuryavanshi/NLP Project/results/roberta-base_test_metrics.csv
  - WARNING: No result file found for 'deberta'
  - Found existing result for 'hatebert' at /WAVE/users2/unix/ysuryavanshi/NLP Project/results/martin-ha_toxic-comment-model_test_metrics.csv
  - Found existing result for 'electra' at /WAVE/users2/unix/ysuryavanshi/NLP Project/results/google_electra-base-discriminator_test_metrics.csv

üìä Generating final comprehensive report for all models...
Creating comprehensive model comparison...
Comparison table saved to /WAVE/users2/unix/ysuryavanshi/NLP Project/results/all_models_comparison.csv

üìä MODEL PERFORMANCE RANKING:
----------------------------------------------------------------------
 2. ROBERTA    | F1-Micro: 0.7235 | F1-Macro: 0.3798 | ROC-AUC: 0.9796
 3. HATEBERT   | F1-Micro: 0.1473 | F1-Macro: 0.0958 | ROC-AUC: 0.9353
 4. ELECTRA    | F1-Micro: 0.0751 | F1-Macro: 0.0708 | ROC-AUC: 0.5385
 1. BERT       | F1-Micro: 0.0745 | F1-Macro: 0.0319 | ROC-AUC: 0.5012
Comprehensive comparison plot saved to /WAVE/users2/unix/ysuryavanshi/NLP Project/plots/comprehensive_transformer_comparison.png

üéâ Process Complete!
Exit status: 0
========================================================
GUARANTEED FINAL RUN FINISHED at Fri Jun  6 06:44:22 PDT 2025
========================================================
